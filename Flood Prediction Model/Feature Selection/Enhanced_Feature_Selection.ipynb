{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 1: Introduction\n",
   "id": "1ab923dc39b3ab16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Enhanced Feature Selection for Water Level Prediction\n",
    "\n",
    "This notebook implements an improved feature selection approach for water level prediction. The goal is to identify the most important features affecting water levels at Hanwella while avoiding selection bias.\n",
    "\n",
    "We'll use multiple feature selection methods:\n",
    "1. **Univariate Feature Selection** using statistical tests\n",
    "2. **Feature Importance** from tree-based models\n",
    "3. **Correlation Analysis** with heatmaps\n",
    "4. **Partial Dependence Plots** for validating feature relationships\n",
    "\n",
    "By combining these methods, we can make more robust feature selection decisions."
   ],
   "id": "4af81f7cd0880a4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 2: Data Loading\n",
   "id": "e5681d5b1e2e5902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Processed_data_with_numerical_24.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(data.head())\n",
    "print(\"\\nColumn datatypes:\")\n",
    "display(data.dtypes)\n"
   ],
   "id": "486ed167c9661fac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 3: Data Preparation\n",
   "id": "620ffee1b69764f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define target column\n",
    "target_column = 'Hanwella_max_next_24h'\n",
    "\n",
    "# Create X and y\n",
    "X = data.drop(columns=[target_column])  # All columns except target\n",
    "y = data[target_column]  # Only target column\n",
    "\n",
    "# Convert datetime columns\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        X[col] = pd.to_datetime(X[col])\n",
    "    except ValueError:\n",
    "        pass  # Handle columns that cannot be converted\n",
    "\n",
    "# Extract numerical features from datetime columns\n",
    "for col in X.select_dtypes(include=['datetime64']).columns:\n",
    "    X[col + '_year'] = X[col].dt.year\n",
    "    X[col + '_month'] = X[col].dt.month\n",
    "    X[col + '_day'] = X[col].dt.day\n",
    "    X = X.drop(columns=[col])  # Remove original datetime column\n",
    "\n",
    "print(\"Features shape after preprocessing:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ],
   "id": "79cb1906999cee6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 4: Descriptive Analysis\n",
   "id": "a85248a3161c5e7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Descriptive Analysis of Features\n",
    "\n",
    "Before selecting features, it's important to understand their distributions, relationships, and statistical properties. This helps in:\n",
    "\n",
    "- Identifying outliers or anomalies\n",
    "- Understanding data distributions\n",
    "- Discovering relationships between features\n",
    "- Finding potential correlations with the target variable\n",
    "- Gaining domain-specific insights\n",
    "\n",
    "The following analyses will help us better understand our dataset and make more informed feature selection decisions."
   ],
   "id": "a1c359558b2e66d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 5: Summary Statistics\n",
   "id": "18d951f35958b1ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate summary statistics\n",
    "summary_stats = X.describe()\n",
    "print(\"Summary statistics for features:\")\n",
    "display(summary_stats)\n",
    "\n",
    "# Target variable statistics\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "display(pd.DataFrame(y.describe()).T)\n",
    "\n",
    "# Display target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y, kde=True)\n",
    "plt.title('Distribution of Target Variable (Hanwella_max_next_24h)')\n",
    "plt.xlabel('Water Level')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ],
   "id": "121b5db96cece3cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 6: Box Plots\n",
   "id": "d1876938bc73f47e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select top numerical features (adjust n_features as needed)\n",
    "n_features = 10\n",
    "numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create box plots for numerical features\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features[:n_features], 1):\n",
    "    plt.subplot(n_features // 2 + n_features % 2, 2, i)\n",
    "    sns.boxplot(x=X[feature])\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots of features vs target (select a few key features)\n",
    "key_features = ['Hanwella_WaterLevel', 'Glencourse_WaterLevel', 'weighted_rainfall_cum_24h']\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(key_features, 1):\n",
    "    plt.subplot(len(key_features), 1, i)\n",
    "    sns.boxplot(x=pd.qcut(X[feature], 4), y=y)\n",
    "    plt.title(f'Target vs {feature} (Quartiles)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "cf29f636528c7855"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 7: Scatter Plots\n",
   "id": "fd0f417577f3564f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select key numerical features\n",
    "key_features = ['Hanwella_WaterLevel', 'Glencourse_WaterLevel',\n",
    "                'weighted_rainfall_cum_24h', 'Hanwella_StreamFlow']\n",
    "\n",
    "# Create scatter plots with target\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(key_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(X[feature], y, alpha=0.5)\n",
    "\n",
    "    # Add trend line\n",
    "    z = np.polyfit(X[feature], y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(X[feature], p(X[feature]), \"r--\", alpha=0.8)\n",
    "\n",
    "    plt.title(f'Scatter Plot: {feature} vs Target')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Target')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create pairplot for key features with target\n",
    "features_with_target = data[key_features + [target_column]]\n",
    "sns.pairplot(features_with_target, height=2.5)\n",
    "plt.suptitle('Pairplot of Key Features with Target', y=1.02)\n",
    "plt.show()\n"
   ],
   "id": "32e96f98a7e05213"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 8: Bar Plots\n",
   "id": "4a3dd1073831b38f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For categorical features or binned numerical features\n",
    "# First, identify if we have any categorical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "if categorical_features:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(categorical_features, 1):\n",
    "        plt.subplot(len(categorical_features), 1, i)\n",
    "        data.groupby(feature)[target_column].mean().plot(kind='bar')\n",
    "        plt.title(f'Mean Target Value by {feature}')\n",
    "        plt.ylabel('Mean Target Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # If no categorical features, bin some numerical ones\n",
    "    key_features = ['Hanwella_WaterLevel', 'weighted_rainfall_cum_24h']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, feature in enumerate(key_features, 1):\n",
    "        plt.subplot(len(key_features), 1, i)\n",
    "        # Create bins and calculate mean target value per bin\n",
    "        bins = pd.qcut(X[feature], 5)\n",
    "        data.groupby(bins)[target_column].mean().plot(kind='bar')\n",
    "        plt.title(f'Mean Target Value by {feature} (Quintiles)')\n",
    "        plt.ylabel('Mean Target Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "e8b97e708f267457"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 9: Correlation Heatmap\n",
   "id": "926090a2929cc145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Create a mask for the upper triangle to avoid redundancy\n",
    "mask = np.triu(np.ones_like(correlation_matrix))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix,\n",
    "            mask=mask,\n",
    "            annot=False,  # Show correlation values (set to False if too many features)\n",
    "            cmap='RdYlGn',  # Red-Yellow-Green colormap\n",
    "            center=0,  # Center the colormap at 0\n",
    "            linewidths=0.5,  # Add grid lines\n",
    "            cbar_kws={\"shrink\": .8})  # Customize colorbar\n",
    "\n",
    "plt.title('Feature Correlation Heatmap', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target specifically\n",
    "target_correlations = correlation_matrix[target_column].sort_values(ascending=False)\n",
    "top_features = target_correlations.drop(target_column).head(15)\n",
    "bottom_features = target_correlations.drop(target_column).tail(5)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Top positive correlations\n",
    "plt.subplot(1, 2, 1)\n",
    "top_features.plot(kind='barh', color='green')\n",
    "plt.title('Top Positive Correlations with Target')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "# Bottom negative correlations\n",
    "plt.subplot(1, 2, 2)\n",
    "bottom_features.plot(kind='barh', color='red')\n",
    "plt.title('Top Negative Correlations with Target')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "8ffaab0b5e4701f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 10: Feature Selection Methods\n",
   "id": "7167d623c1251b86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection Methods\n",
    "\n",
    "We'll use multiple feature selection techniques to identify the most important features:\n",
    "\n",
    "1. **Univariate Selection**: Statistical tests (f_regression) to select features with strongest relationship to target\n",
    "2. **Feature Importance**: Tree-based models to rank features by their contribution\n",
    "3. **Correlation Analysis**: Examining relationships between features and target\n",
    "4. **Partial Dependence Plots**: Visualizing how features affect predictions when accounting for other features\n",
    "\n",
    "Using multiple methods helps avoid bias that might occur when using only one approach."
   ],
   "id": "25a52e2860aaade9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 11: Univariate Feature Selection\n",
   "id": "9c28cf6b0b7163b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Apply SelectKBest with f_regression\n",
    "best_features = SelectKBest(score_func=f_regression, k=20).fit(X, y)\n",
    "feature_scores = best_features.scores_\n",
    "feature_pvalues = best_features.pvalues_\n",
    "\n",
    "# Create and display the feature scores DataFrame\n",
    "featurescores = pd.DataFrame({\n",
    "    'Features': X.columns,\n",
    "    'Score': feature_scores,\n",
    "    'P-value': feature_pvalues\n",
    "})\n",
    "\n",
    "# Sort by score in descending order\n",
    "featurescores = featurescores.sort_values('Score', ascending=False)\n",
    "featurescores = featurescores.reset_index(drop=True)\n",
    "\n",
    "print(\"Top 20 features by F-score:\")\n",
    "display(featurescores.head(20))\n",
    "\n",
    "# Plot scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Score', y='Features', data=featurescores.head(20))\n",
    "plt.title('Top 20 Features Based on F-regression Scores')\n",
    "plt.xlabel('F-Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Function to evaluate different k values\n",
    "def evaluate_k_values(X, y, max_k):\n",
    "    k_values = range(1, max_k + 1)\n",
    "    scores = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Select k best features\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "        # Evaluate with cross-validation\n",
    "        model = LinearRegression()\n",
    "        cv_scores = cross_val_score(model, X_selected, y, cv=5, scoring='r2')\n",
    "        scores.append(cv_scores.mean())\n",
    "\n",
    "    return k_values, scores\n",
    "\n",
    "\n",
    "# Calculate scores for different k values\n",
    "max_k = min(30, len(X.columns))  # Limit to 30 features for computational efficiency\n",
    "k_values, scores = evaluate_k_values(X, y, max_k)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, scores, 'b-', marker='o')\n",
    "plt.xlabel('Number of Features (k)')\n",
    "plt.ylabel('Cross-validation Score (RÂ²)')\n",
    "plt.title('Feature Selection: Performance vs Number of Features')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add vertical line at elbow point\n",
    "try:\n",
    "    scores_diff = np.diff(scores)\n",
    "    elbow_idx = np.where(scores_diff < 0.01)[0][0]\n",
    "    optimal_k = elbow_idx + 1\n",
    "    plt.axvline(x=optimal_k, color='r', linestyle='--',\n",
    "                label=f'Optimal k = {optimal_k}')\n",
    "    plt.legend()\n",
    "except IndexError:\n",
    "    print(\"Could not determine a clear elbow point automatically\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save univariate selected features\n",
    "univariate_features = featurescores['Features'].tolist()[:optimal_k]\n",
    "print(f\"\\nOptimal {optimal_k} features from univariate selection:\")\n",
    "for i, feature in enumerate(univariate_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ],
   "id": "6ab43cd10dca4d8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 12: Tree-Based Feature Importance\n",
   "id": "d795d13ace9acc02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Create and fit the model\n",
    "model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importances = pd.DataFrame({\n",
    "    'Features': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importances = importances.sort_values('Importance', ascending=False)\n",
    "importances = importances.reset_index(drop=True)\n",
    "\n",
    "print(\"Top 20 features by importance:\")\n",
    "display(importances.head(20))\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Features', data=importances.head(20))\n",
    "plt.title('Feature Importance Using ExtraTreesRegressor')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save top tree-based features\n",
    "tree_features = importances['Features'].tolist()[:optimal_k]\n",
    "print(f\"\\nTop {optimal_k} features from tree-based importance:\")\n",
    "for i, feature in enumerate(tree_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ],
   "id": "14163ad7dfa4c8d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 13: Partial Dependence Plots\n",
   "id": "e99a8d1a254cb8dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a model for PDPs (RandomForest works well for this)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Get top features from previous methods\n",
    "# Combine and get unique features from univariate and tree-based methods\n",
    "top_features_combined = list(set(univariate_features[:10] + tree_features[:10]))\n",
    "print(f\"Top features for partial dependence analysis: {len(top_features_combined)}\")\n",
    "for i, feature in enumerate(top_features_combined, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Generate partial dependence plots for individual features\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_partial_dependence(\n",
    "    rf_model,\n",
    "    X,\n",
    "    features=top_features_combined[:8],  # Top 8 features\n",
    "    n_cols=2,\n",
    "    n_jobs=-1,\n",
    "    grid_resolution=50,\n",
    "    ax=ax\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate 2D partial dependence plots for feature interactions\n",
    "if len(top_features_combined) >= 2:\n",
    "    # Select pairs of features for interaction plots\n",
    "    feature_pairs = [(0, 1), (2, 3)]  # Example: pairs of top features\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    plot_partial_dependence(\n",
    "        rf_model,\n",
    "        X,\n",
    "        features=feature_pairs,\n",
    "        n_cols=2,\n",
    "        n_jobs=-1,\n",
    "        grid_resolution=20,\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate feature importance based on PDP variance\n",
    "pdp_importance = {}\n",
    "for feature_idx, feature_name in enumerate(top_features_combined):\n",
    "    # Compute PDP for the feature\n",
    "    pdp_values = partial_dependence(\n",
    "        rf_model,\n",
    "        X,\n",
    "        features=[feature_idx],\n",
    "        grid_resolution=50,\n",
    "        kind='average'\n",
    "    )\n",
    "\n",
    "    # Calculate variance of the PDP\n",
    "    pdp_variance = np.var(pdp_values['average'][0])\n",
    "    pdp_importance[feature_name] = pdp_variance\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "pdp_importance_df = pd.DataFrame({\n",
    "    'Feature': list(pdp_importance.keys()),\n",
    "    'PDP_Variance': list(pdp_importance.values())\n",
    "})\n",
    "pdp_importance_df = pdp_importance_df.sort_values('PDP_Variance', ascending=False)\n",
    "pdp_importance_df = pdp_importance_df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFeature importance based on PDP variance:\")\n",
    "display(pdp_importance_df)\n",
    "\n",
    "# Save top PDP-based features\n",
    "pdp_features = pdp_importance_df['Feature'].tolist()[:optimal_k]\n",
    "print(f\"\\nTop {optimal_k} features from PDP analysis:\")\n",
    "for i, feature in enumerate(pdp_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ],
   "id": "cbec6b2e85d4ece5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 14: Consolidated Feature Selection\n",
   "id": "5d85fd232a2866eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a consolidated feature importance DataFrame\n",
    "all_methods = pd.DataFrame({'Features': X.columns})\n",
    "\n",
    "\n",
    "# Add ranks from different methods\n",
    "def add_method_ranks(df, feature_list, method_name):\n",
    "    ranks = {}\n",
    "    for i, feature in enumerate(feature_list, 1):\n",
    "        ranks[feature] = i\n",
    "\n",
    "    df[f'{method_name}_Rank'] = df['Features'].apply(\n",
    "        lambda x: ranks.get(x, len(df) + 1))\n",
    "    return df\n",
    "\n",
    "\n",
    "all_methods = add_method_ranks(all_methods, univariate_features, 'Univariate')\n",
    "all_methods = add_method_ranks(all_methods, tree_features, 'TreeBased')\n",
    "all_methods = add_method_ranks(all_methods, pdp_features, 'PDP')\n",
    "\n",
    "# Calculate average rank\n",
    "all_methods['Avg_Rank'] = all_methods[\n",
    "    ['Univariate_Rank', 'TreeBased_Rank', 'PDP_Rank']\n",
    "].mean(axis=1)\n",
    "\n",
    "# Sort by average rank\n",
    "all_methods = all_methods.sort_values('Avg_Rank')\n",
    "all_methods = all_methods.reset_index(drop=True)\n",
    "\n",
    "print(\"Feature rankings from all methods:\")\n",
    "display(all_methods.head(20))\n",
    "\n",
    "# Plot the top features by average rank\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='Avg_Rank', y='Features',\n",
    "    data=all_methods.head(20),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top 20 Features by Average Rank Across Methods')\n",
    "plt.xlabel('Average Rank (lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select the final features based on average rank\n",
    "final_k = min(optimal_k, 15)  # Consider computational efficiency\n",
    "final_features = all_methods['Features'].tolist()[:final_k]\n",
    "\n",
    "print(f\"\\nFinal selected {final_k} features:\")\n",
    "for i, feature in enumerate(final_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ],
   "id": "aacca5a359d11e7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 15: Model Evaluation\n",
   "id": "4dc2e60e27f1e7cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Prepare final feature set\n",
    "X_final = X[final_features]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "    # Fit model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'CV R2 Mean': cv_scores.mean(),\n",
    "        'CV R2 Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Compare with baseline (all features)\n",
    "print(\"\\nComparing with baseline (all features):\")\n",
    "X_all_train, X_all_test, y_all_train, y_all_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "baseline_results = []\n",
    "for name, model_class in models.items():\n",
    "    # Create a new instance\n",
    "    model = model_class.__class__()\n",
    "    if hasattr(model, 'random_state'):\n",
    "        model.random_state = 42\n",
    "\n",
    "    model.fit(X_all_train, y_all_train)\n",
    "    y_all_pred = model.predict(X_all_test)\n",
    "\n",
    "    r2_all = r2_score(y_all_test, y_all_pred)\n",
    "    rmse_all = np.sqrt(mean_squared_error(y_all_test, y_all_pred))\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Model': name,\n",
    "        'R2 (all features)': r2_all,\n",
    "        'RMSE (all features)': rmse_all,\n",
    "        'R2 (selected features)': results[list(models.keys()).index(name)]['R2'],\n",
    "        'RMSE (selected features)': results[list(models.keys()).index(name)]['RMSE']\n",
    "    })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "display(baseline_df)\n"
   ],
   "id": "6bc95c168cef5d94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 16: Conclusion\n",
   "id": "51e714f893caf7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "Our comprehensive feature selection process involved multiple techniques:\n",
    "1. Univariate Selection\n",
    "2. Tree-based Feature Importance\n",
    "3. Correlation Analysis\n",
    "4. Partial Dependence Plots\n",
    "\n",
    "By combining these approaches, we've identified a robust set of features for predicting water levels:\n",
    "- [List top features here]\n",
    "\n",
    "The advantages of our approach include:\n",
    "- **Reduced bias** by using multiple selection methods\n",
    "- **Better model performance** with optimized feature set\n",
    "- **Deeper understanding** of feature relationships through descriptive analysis\n",
    "\n",
    "### Performance Improvements\n",
    "\n",
    "Our selected feature set achieves:\n",
    "- Similar or better model performance compared to using all features\n",
    "- Reduced computational complexity\n",
    "- More interpretable models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Fine-tune model hyperparameters using the selected features\n",
    "2. Consider feature engineering to create new combined features\n",
    "3. Explore temporal aspects of the data (time series analysis)\n",
    "4. Deploy model with selected features for production use"
   ],
   "id": "39d4beb357fba81c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3dc54d85117253a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
